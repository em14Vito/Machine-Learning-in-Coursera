## 1
![](https://github.com/em14Vito/Machine-Learning-in-Coursera/blob/master/week2/img/l1.png)
#Ans:
![](https://github.com/em14Vito/Machine-Learning-in-Coursera/blob/master/week2/img/l1_a.png)
##2
![](https://github.com/em14Vito/Machine-Learning-in-Coursera/blob/master/week2/img/l2.png)
#Ans:
<strong>Rather than use the current value of α, it'd be more promising to try a larger value of α (say α=1.0).</strong>
##3
![](https://github.com/em14Vito/Machine-Learning-in-Coursera/blob/master/week2/img/l3.png)
#Ans:
<strong>X is 14×4, y is 14×1, θ is 4×1</strong>
##4
![](https://github.com/em14Vito/Machine-Learning-in-Coursera/blob/master/week2/img/l4.png)
#Ans:
<strong>Gradient descent, since (XTX)^−1 will be very slow to compute in the normal equation.</strong>
##5
![](https://github.com/em14Vito/Machine-Learning-in-Coursera/blob/master/week2/img/l4.png)
#Ans:
<strong>It speeds up gradient descent by making it require fewer iterations to get to a good solution.</strong>
